{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29480\\1238725948.py:21: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1, header=None)\n",
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29480\\1238725948.py:24: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(file2, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the merged DataFrame: (2540047, 49)\n",
      "Merged file saved to: C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/RAW_Merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths for each of the four CSV files\n",
    "file1 = r\"C:\\Users\\ishan\\OneDrive\\Desktop\\Research\\Dataset\\UNSW-NB15\\CVS\\UNSW-NB15_1.csv\"\n",
    "file2 = r\"C:\\Users\\ishan\\OneDrive\\Desktop\\Research\\Dataset\\UNSW-NB15\\CVS\\UNSW-NB15_2.csv\"\n",
    "file3 = r\"C:\\Users\\ishan\\OneDrive\\Desktop\\Research\\Dataset\\UNSW-NB15\\CVS\\UNSW-NB15_3.csv\"\n",
    "file4 = r\"C:\\Users\\ishan\\OneDrive\\Desktop\\Research\\Dataset\\UNSW-NB15\\CVS\\UNSW-NB15_4.csv\"\n",
    "\n",
    "# Define the column names as specified\n",
    "column_names = [\n",
    "    \"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\",\n",
    "    \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\", \"Dpkts\",\n",
    "    \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\",\n",
    "    \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\", \"synack\", \"ackdat\",\n",
    "    \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\",\n",
    "    \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\",\n",
    "    \"ct_dst_src_ltm\", \"attack_cat\", \"Label\"\n",
    "]\n",
    "\n",
    "# Load each CSV file and assign column names\n",
    "df1 = pd.read_csv(file1, header=None)\n",
    "df1.columns = column_names\n",
    "\n",
    "df2 = pd.read_csv(file2, header=None)\n",
    "df2.columns = column_names\n",
    "\n",
    "df3 = pd.read_csv(file3, header=None)\n",
    "df3.columns = column_names\n",
    "\n",
    "df4 = pd.read_csv(file4, header=None)\n",
    "df4.columns = column_names\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "merged_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Check the shape of the merged DataFrame\n",
    "print(\"Shape of the merged DataFrame:\", merged_df.shape)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "output_path = r\"C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/RAW_Merged.csv\"\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29480\\603969921.py:21: DtypeWarning: Columns (1,3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning complete. Cleaned data saved to C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def clean_unsw_nb15_csv(input_file, output_file):\n",
    "    # Step 2: Standardize attack categories\n",
    "    value_variations = {\n",
    "        'Normal': ['', ' '],\n",
    "        'Exploits': ['Exploits ', ' Exploits', ' Exploits ', 'Exploit', 'Exploit ', ' Exploit', ' Exploit '],\n",
    "        'Shellcode': ['Shellcode ', ' Shellcode', ' Shellcode '],\n",
    "        'Backdoor': ['Backdoor ', ' Backdoor', ' Backdoor ', 'Backdoors', 'Backdoors ', ' Backdoors', ' Backdoors '],\n",
    "        'Fuzzers': ['Fuzzers ', ' Fuzzers', ' Fuzzers '],\n",
    "        'Analysis': ['Analysis ', ' Analysis', ' Analysis '],\n",
    "        'Reconnaissance': ['Reconnaissance ', ' Reconnaissance', ' Reconnaissance '],\n",
    "        'Worms': ['Worms ', ' Worms', ' Worms '],\n",
    "        'DoS': ['DoS ', ' DoS', ' DoS '],\n",
    "        'Generic': ['Generic ', ' Generic', ' Generic ']\n",
    "    }\n",
    "    \n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Standardize attack categories\n",
    "    for unique_value, variations in value_variations.items():\n",
    "        df['attack_cat'] = df['attack_cat'].replace(variations, unique_value)\n",
    "    \n",
    "    # Step 3: Remove redundant rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Step 4: Handle null/blank values\n",
    "    df.replace(['', ' ', '\\n', '\\t'], 0, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Save cleaned DataFrame to new CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data cleaning complete. Cleaned data saved to {output_file}.\")\n",
    "\n",
    "# Usage\n",
    "input_file = r\"C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/RAW_Merged.csv\"  # Path to your merged CSV\n",
    "output_file = r\"C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15.csv\"  # Path for cleaned CSV\n",
    "clean_unsw_nb15_csv(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29480\\101275.py:5: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cleaned_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column in the cleaned dataset:\n",
      "srcip                object\n",
      "sport                object\n",
      "dstip                object\n",
      "dsport               object\n",
      "proto                object\n",
      "state                object\n",
      "dur                 float64\n",
      "sbytes                int64\n",
      "dbytes                int64\n",
      "sttl                  int64\n",
      "dttl                  int64\n",
      "sloss                 int64\n",
      "dloss                 int64\n",
      "service              object\n",
      "Sload               float64\n",
      "Dload               float64\n",
      "Spkts                 int64\n",
      "Dpkts                 int64\n",
      "swin                  int64\n",
      "dwin                  int64\n",
      "stcpb                 int64\n",
      "dtcpb                 int64\n",
      "smeansz               int64\n",
      "dmeansz               int64\n",
      "trans_depth           int64\n",
      "res_bdy_len           int64\n",
      "Sjit                float64\n",
      "Djit                float64\n",
      "Stime                 int64\n",
      "Ltime                 int64\n",
      "Sintpkt             float64\n",
      "Dintpkt             float64\n",
      "tcprtt              float64\n",
      "synack              float64\n",
      "ackdat              float64\n",
      "is_sm_ips_ports       int64\n",
      "ct_state_ttl          int64\n",
      "ct_flw_http_mthd    float64\n",
      "is_ftp_login        float64\n",
      "ct_ftp_cmd            int64\n",
      "ct_srv_src            int64\n",
      "ct_srv_dst            int64\n",
      "ct_dst_ltm            int64\n",
      "ct_src_ltm            int64\n",
      "ct_src_dport_ltm      int64\n",
      "ct_dst_sport_ltm      int64\n",
      "ct_dst_src_ltm        int64\n",
      "attack_cat           object\n",
      "Label                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "cleaned_file = r\"C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15.csv\"\n",
    "df = pd.read_csv(cleaned_file)\n",
    "\n",
    "# Display data types of each column\n",
    "print(\"Data types of each column in the cleaned dataset:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29480\\2494052873.py:5: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Column  Unique Values\n",
      "0       srcip             43\n",
      "1       sport         104542\n",
      "2       dstip             47\n",
      "3      dsport         128288\n",
      "4       proto            135\n",
      "5       state             16\n",
      "6     service             13\n",
      "7  attack_cat             11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify the unique value counts for each object-type column\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "unique_counts = {col: df[col].nunique() for col in object_columns}\n",
    "\n",
    "# Convert to DataFrame for better readability and print\n",
    "unique_counts_df = pd.DataFrame(list(unique_counts.items()), columns=['Column', 'Unique Values'])\n",
    "print(unique_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_29480\\4026978432.py:6: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in Object Columns:\n",
      "       Column  Unique Values\n",
      "0       srcip             43\n",
      "1       sport         104542\n",
      "2       dstip             47\n",
      "3      dsport         128288\n",
      "4       proto            135\n",
      "5       state             16\n",
      "6     service             13\n",
      "7  attack_cat             11\n",
      "\n",
      "Number of columns after feature engineering: 49\n",
      "Columns dropped: ['proto', 'state', 'service', 'sport', 'dsport']\n",
      "\n",
      "Updated DataFrame columns:\n",
      "Index(['srcip', 'dstip', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit',\n",
      "       'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'Label', 'proto_state_interaction', 'proto_grouped',\n",
      "       'service_grouped', 'sport_binned', 'dsport_binned'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Initial Exploration - Identify Unique Values in Object Columns\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "unique_counts = {col: df[col].nunique() for col in object_columns}\n",
    "unique_counts_df = pd.DataFrame(list(unique_counts.items()), columns=['Column', 'Unique Values'])\n",
    "print(\"Unique Values in Object Columns:\")\n",
    "print(unique_counts_df)\n",
    "\n",
    "# Step 2: Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "label_encode_cols = ['proto', 'state', 'service', 'attack_cat']\n",
    "for col in label_encode_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Step 3: Interaction Term - Creating `proto_state_interaction`\n",
    "df['proto_state_interaction'] = df['proto'].astype(str) + \"_\" + df['state'].astype(str)\n",
    "df['proto_state_interaction'] = label_encoder.fit_transform(df['proto_state_interaction'])\n",
    "\n",
    "# Step 4: Hierarchical Grouping for `proto` and `service`\n",
    "def group_proto(proto):\n",
    "    if proto in ['tcp', 'http', 'https', 'ftp']:  # Update as needed\n",
    "        return 'TCP'\n",
    "    elif proto in ['udp', 'dns']:  # Update as needed\n",
    "        return 'UDP'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def group_service(service):\n",
    "    if service in ['http', 'https', 'web']:  # Update as needed\n",
    "        return 'Web'\n",
    "    elif service in ['ftp', 'sftp', 'ftps']:  # Update as needed\n",
    "        return 'File_Transfer'\n",
    "    elif service in ['dns']:\n",
    "        return 'DNS'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['proto_grouped'] = df['proto'].apply(group_proto)\n",
    "df['service_grouped'] = df['service'].apply(group_service)\n",
    "\n",
    "# Encode grouped columns\n",
    "df['proto_grouped'] = label_encoder.fit_transform(df['proto_grouped'])\n",
    "df['service_grouped'] = label_encoder.fit_transform(df['service_grouped'])\n",
    "\n",
    "# Step 5: Binning for `sport` and `dsport`\n",
    "def port_binning(port):\n",
    "    try:\n",
    "        if isinstance(port, str) and port.startswith('0x'):\n",
    "            port = int(port, 16)\n",
    "        else:\n",
    "            port = int(port)\n",
    "    except ValueError:\n",
    "        return 'unknown'\n",
    "    \n",
    "    if 0 <= port <= 1023:\n",
    "        return 'well_known'\n",
    "    elif 1024 <= port <= 49151:\n",
    "        return 'registered'\n",
    "    elif 49152 <= port <= 65535:\n",
    "        return 'dynamic'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "df['sport_binned'] = df['sport'].apply(port_binning)\n",
    "df['dsport_binned'] = df['dsport'].apply(port_binning)\n",
    "df['sport_binned'] = label_encoder.fit_transform(df['sport_binned'])\n",
    "df['dsport_binned'] = label_encoder.fit_transform(df['dsport_binned'])\n",
    "\n",
    "# Step 6: Dropping Unnecessary Columns\n",
    "columns_dropped = ['proto', 'state', 'service', 'sport', 'dsport']\n",
    "df = df.drop(columns_dropped, axis=1)\n",
    "\n",
    "# Final Output - Display Number of Columns and Dropped Columns\n",
    "print(f\"\\nNumber of columns after feature engineering: {df.shape[1]}\")\n",
    "print(\"Columns dropped:\", columns_dropped)\n",
    "print(\"\\nUpdated DataFrame columns:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv('C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15_Feature_Engineered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of columns after feature engineering: 68\n",
      "Updated DataFrame columns:\n",
      "Index(['srcip', 'dstip', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit',\n",
      "       'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'Label', 'proto_state_interaction', 'proto_grouped',\n",
      "       'service_grouped', 'sport_binned', 'dsport_binned', 'byte_ratio',\n",
      "       'packet_ratio', 'source_packet_rate', 'destination_packet_rate',\n",
      "       'source_byte_rate', 'destination_byte_rate', 'jit_mean', 'jit_diff',\n",
      "       'total_load', 'load_ratio', 'ttl_diff', 'loss_diff', 'window_diff',\n",
      "       'packet_duration_ratio', 'byte_packet_ratio_src',\n",
      "       'byte_packet_ratio_dst', 'byte_size_mean', 'byte_size_diff',\n",
      "       'session_duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Cleaned_UNSW_NB15_Feature_Engineered.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Byte and Packet Ratios\n",
    "df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)  # Avoid division by zero\n",
    "df['packet_ratio'] = df['Spkts'] / (df['Dpkts'] + 1)\n",
    "\n",
    "# 2. Packet and Byte Rates\n",
    "df['source_packet_rate'] = df['Spkts'] / (df['dur'] + 1)  # Packets per second from source\n",
    "df['destination_packet_rate'] = df['Dpkts'] / (df['dur'] + 1)  # Packets per second to destination\n",
    "df['source_byte_rate'] = df['sbytes'] / (df['dur'] + 1)  # Bytes per second from source\n",
    "df['destination_byte_rate'] = df['dbytes'] / (df['dur'] + 1)  # Bytes per second to destination\n",
    "\n",
    "# 3. Inter-Packet Jitter Calculations\n",
    "df['jit_mean'] = (df['Sjit'] + df['Djit']) / 2\n",
    "df['jit_diff'] = df['Sjit'] - df['Djit']\n",
    "\n",
    "# 4. Load Features\n",
    "df['total_load'] = df['Sload'] + df['Dload']\n",
    "df['load_ratio'] = df['Sload'] / (df['Dload'] + 1)\n",
    "\n",
    "# 5. Interaction Terms for TTL, Losses, and Window Sizes\n",
    "df['ttl_diff'] = df['sttl'] - df['dttl']\n",
    "df['loss_diff'] = df['sloss'] - df['dloss']\n",
    "df['window_diff'] = df['swin'] - df['dwin']\n",
    "\n",
    "# 6. Duration and Packet Interaction Features\n",
    "df['packet_duration_ratio'] = df['dur'] / (df['Spkts'] + df['Dpkts'] + 1)\n",
    "df['byte_packet_ratio_src'] = df['sbytes'] / (df['Spkts'] + 1)\n",
    "df['byte_packet_ratio_dst'] = df['dbytes'] / (df['Dpkts'] + 1)\n",
    "\n",
    "# 7. Aggregated Bytes and Packet Size Ratios\n",
    "df['byte_size_mean'] = (df['smeansz'] + df['dmeansz']) / 2\n",
    "df['byte_size_diff'] = df['smeansz'] - df['dmeansz']\n",
    "\n",
    "# 8. Timing Features (for potential time-based behavior)\n",
    "df['session_duration'] = df['Ltime'] - df['Stime']\n",
    "\n",
    "# Display Final Dataset Columns and Number of Columns\n",
    "print(f\"\\nNumber of columns after feature engineering: {df.shape[1]}\")\n",
    "print(\"Updated DataFrame columns:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv('C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Enhanced_UNSW_NB15.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of columns after additional feature engineering: 77\n",
      "Updated DataFrame columns:\n",
      "Index(['srcip', 'dstip', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit',\n",
      "       'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'Label', 'proto_state_interaction', 'proto_grouped',\n",
      "       'service_grouped', 'sport_binned', 'dsport_binned', 'byte_ratio',\n",
      "       'packet_ratio', 'source_packet_rate', 'destination_packet_rate',\n",
      "       'source_byte_rate', 'destination_byte_rate', 'jit_mean', 'jit_diff',\n",
      "       'total_load', 'load_ratio', 'ttl_diff', 'loss_diff', 'window_diff',\n",
      "       'packet_duration_ratio', 'byte_packet_ratio_src',\n",
      "       'byte_packet_ratio_dst', 'byte_size_mean', 'byte_size_diff',\n",
      "       'session_duration', 'byte_entropy', 'Sintpkt_std', 'Dintpkt_std',\n",
      "       'hour', 'day_of_week', 'Sintpkt_var', 'Dintpkt_var',\n",
      "       'packet_count_change_rate', 'byte_count_change_rate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Enhanced_UNSW_NB15.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# 1. Entropy-Based Features\n",
    "# Calculate entropy for byte size distribution\n",
    "df['byte_entropy'] = df.apply(lambda row: entropy([row['sbytes'], row['dbytes']]), axis=1)\n",
    "\n",
    "# 2. Statistical Features\n",
    "# Standard deviation of inter-packet times\n",
    "df['Sintpkt_std'] = df['Sintpkt'].rolling(window=3, min_periods=1).std()  # Adjust window as needed\n",
    "df['Dintpkt_std'] = df['Dintpkt'].rolling(window=3, min_periods=1).std()\n",
    "\n",
    "# 3. Time of Day Features\n",
    "# Extract hour and day of week from start time (assuming 'Stime' is in UNIX timestamp format)\n",
    "df['hour'] = pd.to_datetime(df['Stime'], unit='s').dt.hour\n",
    "df['day_of_week'] = pd.to_datetime(df['Stime'], unit='s').dt.dayofweek\n",
    "\n",
    "# 4. TCP Flag Aggregation (assuming flags are in separate columns; create or adjust as necessary)\n",
    "# Example assuming 'ACK', 'SYN', and 'FIN' columns exist (replace with actual flag columns if needed)\n",
    "if 'ACK' in df.columns and 'SYN' in df.columns and 'FIN' in df.columns:\n",
    "    df['tcp_flag_sum'] = df['ACK'] + df['SYN'] + df['FIN']  # Summing flags per row\n",
    "\n",
    "# 5. Connection Stability Features\n",
    "# Variance of packet inter-arrival times\n",
    "df['Sintpkt_var'] = df['Sintpkt'].rolling(window=5, min_periods=1).var()\n",
    "df['Dintpkt_var'] = df['Dintpkt'].rolling(window=5, min_periods=1).var()\n",
    "\n",
    "# 6. Relative Change Features\n",
    "# Rate of change in packet count and byte count\n",
    "df['packet_count_change_rate'] = df['Spkts'].diff() / (df['dur'] + 1)\n",
    "df['byte_count_change_rate'] = df['sbytes'].diff() / (df['dur'] + 1)\n",
    "\n",
    "# Display Final Dataset Columns and Number of Columns\n",
    "print(f\"\\nNumber of columns after additional feature engineering: {df.shape[1]}\")\n",
    "print(\"Updated DataFrame columns:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv('C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Fully_Enhanced_UNSW_NB15.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        srcip          dstip           dur    sbytes    dbytes      sttl  \\\n",
      "0  59.166.0.0  149.171.126.6  1.200687e-07  0.000009  0.000011  0.121569   \n",
      "1  59.166.0.0  149.171.126.9  4.112267e-06  0.000037  0.000021  0.121569   \n",
      "2  59.166.0.6  149.171.126.7  1.273525e-07  0.000010  0.000012  0.121569   \n",
      "3  59.166.0.5  149.171.126.5  1.375953e-07  0.000009  0.000011  0.121569   \n",
      "4  59.166.0.3  149.171.126.0  1.330429e-07  0.000010  0.000012  0.121569   \n",
      "\n",
      "       dttl  sloss  dloss     Sload  ...  session_duration  byte_entropy  \\\n",
      "0  0.114173    0.0    0.0  0.000084  ...               0.0      0.991553   \n",
      "1  0.114173    0.0    0.0  0.000015  ...               0.0      0.947062   \n",
      "2  0.114173    0.0    0.0  0.000087  ...               0.0      0.992952   \n",
      "3  0.114173    0.0    0.0  0.000073  ...               0.0      0.991553   \n",
      "4  0.114173    0.0    0.0  0.000083  ...               0.0      0.992952   \n",
      "\n",
      "    Sintpkt_std   Dintpkt_std      hour  day_of_week   Sintpkt_var  \\\n",
      "0           NaN           NaN  0.478261          0.5           NaN   \n",
      "1  1.014937e-04  1.554751e-04  0.478261          0.5  1.715907e-08   \n",
      "2  8.286923e-05  1.269449e-04  0.478261          0.5  1.143938e-08   \n",
      "3  8.271549e-05  1.269365e-04  0.478261          0.5  8.558375e-09   \n",
      "4  3.989915e-07  1.771141e-07  0.478261          0.5  6.856891e-09   \n",
      "\n",
      "    Dintpkt_var  packet_count_change_rate  byte_count_change_rate  \n",
      "0           NaN                       NaN                     NaN  \n",
      "1  4.029134e-08                  0.923011                0.925505  \n",
      "2  2.686089e-08                  0.922634                0.925451  \n",
      "3  2.014389e-08                  0.922826                0.925477  \n",
      "4  1.612617e-08                  0.922826                0.925479  \n",
      "\n",
      "[5 rows x 77 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Fully_Enhanced_UNSW_NB15.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Exclude columns that donâ€™t need scaling, such as categorical or ID columns\n",
    "# Adjust based on your dataset's needs\n",
    "columns_to_scale = df.select_dtypes(include=['float64', 'int64']).columns.difference(['Label', 'attack_cat'])\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Display scaled columns to verify\n",
    "print(df.head())\n",
    "\n",
    "# Save the scaled dataset\n",
    "df.to_csv('C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        srcip          dstip           dur    sbytes    dbytes      sttl  \\\n",
      "0  59.166.0.0  149.171.126.6  1.200687e-07  0.000009  0.000011  0.121569   \n",
      "1  59.166.0.0  149.171.126.9  4.112267e-06  0.000037  0.000021  0.121569   \n",
      "2  59.166.0.6  149.171.126.7  1.273525e-07  0.000010  0.000012  0.121569   \n",
      "3  59.166.0.5  149.171.126.5  1.375953e-07  0.000009  0.000011  0.121569   \n",
      "4  59.166.0.3  149.171.126.0  1.330429e-07  0.000010  0.000012  0.121569   \n",
      "\n",
      "       dttl  sloss  dloss     Sload  ...  session_duration  byte_entropy  \\\n",
      "0  0.114173    0.0    0.0  0.000084  ...               0.0      0.991553   \n",
      "1  0.114173    0.0    0.0  0.000015  ...               0.0      0.947062   \n",
      "2  0.114173    0.0    0.0  0.000087  ...               0.0      0.992952   \n",
      "3  0.114173    0.0    0.0  0.000073  ...               0.0      0.991553   \n",
      "4  0.114173    0.0    0.0  0.000083  ...               0.0      0.992952   \n",
      "\n",
      "    Sintpkt_std   Dintpkt_std      hour  day_of_week   Sintpkt_var  \\\n",
      "0  0.000000e+00  0.000000e+00  0.478261          0.5  0.000000e+00   \n",
      "1  1.014937e-04  1.554751e-04  0.478261          0.5  1.715907e-08   \n",
      "2  8.286923e-05  1.269449e-04  0.478261          0.5  1.143938e-08   \n",
      "3  8.271549e-05  1.269365e-04  0.478261          0.5  8.558375e-09   \n",
      "4  3.989915e-07  1.771141e-07  0.478261          0.5  6.856891e-09   \n",
      "\n",
      "    Dintpkt_var  packet_count_change_rate  byte_count_change_rate  \n",
      "0  0.000000e+00                  0.000000                0.000000  \n",
      "1  4.029134e-08                  0.923011                0.925505  \n",
      "2  2.686089e-08                  0.922634                0.925451  \n",
      "3  2.014389e-08                  0.922826                0.925477  \n",
      "4  1.612617e-08                  0.922826                0.925479  \n",
      "\n",
      "[5 rows x 77 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Impute NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv('C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv', index=False)\n",
    "\n",
    "# Display the DataFrame to confirm NaN values have been replaced\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
      "Selected Feature Names: Index(['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload',\n",
      "       'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'smeansz',\n",
      "       'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Stime',\n",
      "       'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
      "       'ct_state_ttl', 'ct_flw_http_mthd', 'ct_srv_src', 'ct_srv_dst',\n",
      "       'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
      "       'ct_dst_src_ltm', 'attack_cat', 'proto_state_interaction',\n",
      "       'sport_binned', 'dsport_binned', 'byte_ratio', 'packet_ratio',\n",
      "       'source_packet_rate', 'destination_packet_rate', 'source_byte_rate',\n",
      "       'destination_byte_rate', 'jit_mean', 'jit_diff', 'total_load',\n",
      "       'load_ratio', 'ttl_diff', 'loss_diff', 'window_diff',\n",
      "       'packet_duration_ratio', 'byte_packet_ratio_src',\n",
      "       'byte_packet_ratio_dst', 'byte_size_mean', 'byte_size_diff',\n",
      "       'session_duration', 'byte_entropy', 'Sintpkt_std', 'Dintpkt_std',\n",
      "       'hour', 'day_of_week', 'Sintpkt_var', 'Dintpkt_var',\n",
      "       'packet_count_change_rate', 'byte_count_change_rate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 10 for score in relevance_scores]  # Scaling relevance scores\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix\n",
    "lambda_param = 0.05  # Reduced penalty for feature selection to encourage selection\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores for Each Feature:\n",
      "Feature dur: 0.08916758723890994\n",
      "Feature sbytes: 0.155220151139704\n",
      "Feature dbytes: 0.13334314761982602\n",
      "Feature sttl: 0.3965692320958887\n",
      "Feature dttl: 0.37954315271458905\n",
      "Feature sloss: 0.05328760654137876\n",
      "Feature dloss: 0.042059182512946824\n",
      "Feature Sload: 0.1063764133000713\n",
      "Feature Dload: 0.10051812994950615\n",
      "Feature Spkts: 0.05476403129593277\n",
      "Feature Dpkts: 0.09184352942342566\n",
      "Feature swin: 0.1909595775257752\n",
      "Feature dwin: 0.19100911147247857\n",
      "Feature stcpb: 0.005881333566255975\n",
      "Feature dtcpb: 0.0038987135141275164\n",
      "Feature smeansz: 0.10584240031275627\n",
      "Feature dmeansz: 0.12110249382859561\n",
      "Feature trans_depth: 0.01177518034180669\n",
      "Feature res_bdy_len: 0.01657983767465132\n",
      "Feature Sjit: 0.04985548485318736\n",
      "Feature Djit: 0.0400583333395077\n",
      "Feature Stime: 0.03248118407795331\n",
      "Feature Ltime: 0.03307012906874185\n",
      "Feature Sintpkt: 0.058600387451930525\n",
      "Feature Dintpkt: 0.10011836942215013\n",
      "Feature tcprtt: 0.09188787506538665\n",
      "Feature synack: 0.08967952360401155\n",
      "Feature ackdat: 0.09072292580303021\n",
      "Feature is_sm_ips_ports: 0.0\n",
      "Feature ct_state_ttl: 0.15778763052264688\n",
      "Feature ct_flw_http_mthd: 0.011239902670815871\n",
      "Feature is_ftp_login: 0.0010109038004495474\n",
      "Feature ct_ftp_cmd: 0.0011712727411943291\n",
      "Feature ct_srv_src: 0.035012634438294166\n",
      "Feature ct_srv_dst: 0.04102802007799666\n",
      "Feature ct_dst_ltm: 0.06691310536939477\n",
      "Feature ct_src_ltm: 0.06210774210613734\n",
      "Feature ct_src_dport_ltm: 0.016295551885569637\n",
      "Feature ct_dst_sport_ltm: 0.01965449678535136\n",
      "Feature ct_dst_src_ltm: 0.0258719400698868\n",
      "Feature attack_cat: 0.19477776261362278\n",
      "Feature proto_state_interaction: 0.28028467293665305\n",
      "Feature proto_grouped: 0.0\n",
      "Feature service_grouped: 0.00026815291334458813\n",
      "Feature sport_binned: 0.20689447870121425\n",
      "Feature dsport_binned: 0.18248353947771712\n",
      "Feature byte_ratio: 0.1566232261699373\n",
      "Feature packet_ratio: 0.1241416223303734\n",
      "Feature source_packet_rate: 0.09834562690846149\n",
      "Feature destination_packet_rate: 0.10149063546774117\n",
      "Feature source_byte_rate: 0.11287565851181092\n",
      "Feature destination_byte_rate: 0.10132496331333618\n",
      "Feature jit_mean: 0.0530326484461241\n",
      "Feature jit_diff: 0.07370709943121079\n",
      "Feature total_load: 0.11573483507623195\n",
      "Feature load_ratio: 0.12289375062510333\n",
      "Feature ttl_diff: 0.36034548276187195\n",
      "Feature loss_diff: 0.1273587643316082\n",
      "Feature window_diff: 0.2748720585447233\n",
      "Feature packet_duration_ratio: 0.10079860118560702\n",
      "Feature byte_packet_ratio_src: 0.16847962941595385\n",
      "Feature byte_packet_ratio_dst: 0.1416412965426359\n",
      "Feature byte_size_mean: 0.1273837419326237\n",
      "Feature byte_size_diff: 0.1408564612431804\n",
      "Feature session_duration: 0.005890372798833532\n",
      "Feature byte_entropy: 0.14548900993146274\n",
      "Feature Sintpkt_std: 0.040718955264243295\n",
      "Feature Dintpkt_std: 0.04887712037816616\n",
      "Feature hour: 0.0440175438083783\n",
      "Feature day_of_week: 0.139985531363656\n",
      "Feature Sintpkt_var: 0.02611193027758074\n",
      "Feature Dintpkt_var: 0.03556513551266938\n",
      "Feature packet_count_change_rate: 0.06377144662533452\n",
      "Feature byte_count_change_rate: 0.05827991855299386\n"
     ]
    }
   ],
   "source": [
    "print(\"Mutual Information Scores for Each Feature:\")\n",
    "for i, score in enumerate(relevance_scores):\n",
    "    print(f\"Feature {features.columns[i]}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 33, 34, 35, 36, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
      "Selected Feature Names: Index(['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload',\n",
      "       'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'smeansz', 'dmeansz', 'Sjit',\n",
      "       'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'ct_state_ttl', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_dst_src_ltm', 'attack_cat', 'proto_state_interaction',\n",
      "       'sport_binned', 'dsport_binned', 'byte_ratio', 'packet_ratio',\n",
      "       'source_packet_rate', 'destination_packet_rate', 'source_byte_rate',\n",
      "       'destination_byte_rate', 'jit_mean', 'jit_diff', 'total_load',\n",
      "       'load_ratio', 'ttl_diff', 'loss_diff', 'window_diff',\n",
      "       'packet_duration_ratio', 'byte_packet_ratio_src',\n",
      "       'byte_packet_ratio_dst', 'byte_size_mean', 'byte_size_diff',\n",
      "       'byte_entropy', 'Sintpkt_std', 'Dintpkt_std', 'hour', 'day_of_week',\n",
      "       'Sintpkt_var', 'Dintpkt_var', 'packet_count_change_rate',\n",
      "       'byte_count_change_rate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 10 for score in relevance_scores]  # Scaling relevance scores\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix\n",
    "lambda_param = 0.2  # Increased penalty for feature selection to reduce selected features\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "Selected Feature Names: Index(['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'Sload', 'Dload',\n",
      "       'Spkts', 'Dpkts', 'swin', 'dwin', 'smeansz', 'dmeansz', 'Sintpkt',\n",
      "       'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'ct_state_ttl', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'attack_cat', 'proto_state_interaction', 'sport_binned',\n",
      "       'dsport_binned', 'byte_ratio', 'packet_ratio', 'source_packet_rate',\n",
      "       'destination_packet_rate', 'source_byte_rate', 'destination_byte_rate',\n",
      "       'jit_mean', 'jit_diff', 'total_load', 'load_ratio', 'ttl_diff',\n",
      "       'loss_diff', 'window_diff', 'packet_duration_ratio',\n",
      "       'byte_packet_ratio_src', 'byte_packet_ratio_dst', 'byte_size_mean',\n",
      "       'byte_size_diff', 'byte_entropy', 'day_of_week',\n",
      "       'packet_count_change_rate', 'byte_count_change_rate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 10 for score in relevance_scores]  # Scaling relevance scores\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Optional: Filter out features with very low mutual information scores\n",
    "min_relevance_threshold = 0.05  # Set threshold for minimum relevance score\n",
    "filtered_features = [col for col, score in zip(features.columns, relevance_scores) if score > min_relevance_threshold]\n",
    "features = features[filtered_features]\n",
    "\n",
    "# Re-calculate mutual information scores after filtering\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 10 for score in relevance_scores]  # Scaling relevance scores\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix after filtering\n",
    "lambda_param = 0.3  # Further increased penalty for feature selection to limit the subset size\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "Selected Feature Names: Index(['sbytes', 'dbytes', 'sttl', 'dttl', 'Sload', 'Dload', 'swin', 'dwin',\n",
      "       'smeansz', 'dmeansz', 'Dintpkt', 'ct_state_ttl', 'attack_cat',\n",
      "       'proto_state_interaction', 'sport_binned', 'dsport_binned',\n",
      "       'byte_ratio', 'packet_ratio', 'destination_packet_rate',\n",
      "       'source_byte_rate', 'destination_byte_rate', 'total_load', 'load_ratio',\n",
      "       'ttl_diff', 'loss_diff', 'window_diff', 'packet_duration_ratio',\n",
      "       'byte_packet_ratio_src', 'byte_packet_ratio_dst', 'byte_size_mean',\n",
      "       'byte_size_diff', 'byte_entropy', 'day_of_week'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 5 for score in relevance_scores]  # Reduced scaling factor\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Optional: Filter out features with a higher minimum mutual information score\n",
    "min_relevance_threshold = 0.1  # Higher threshold to focus on top-relevance features\n",
    "filtered_features = [col for col, score in zip(features.columns, relevance_scores) if score > min_relevance_threshold]\n",
    "features = features[filtered_features]\n",
    "\n",
    "# Re-calculate mutual information scores after filtering\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 5 for score in relevance_scores]  # Apply reduced scaling\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix after filtering\n",
    "lambda_param = 0.4  # Increased penalty for further limiting selected features\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Selected Feature Names: Index(['sbytes', 'sttl', 'dttl', 'swin', 'dwin', 'ct_state_ttl', 'attack_cat',\n",
      "       'proto_state_interaction', 'sport_binned', 'dsport_binned',\n",
      "       'byte_ratio', 'ttl_diff', 'window_diff', 'byte_packet_ratio_src'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 5 for score in relevance_scores]  # Keep scaling factor at 5\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Optional: Filter out features with an even higher minimum mutual information score\n",
    "min_relevance_threshold = 0.15  # Set a higher threshold to focus on top-relevance features\n",
    "filtered_features = [col for col, score in zip(features.columns, relevance_scores) if score > min_relevance_threshold]\n",
    "features = features[filtered_features]\n",
    "\n",
    "# Re-calculate mutual information scores after filtering\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 5 for score in relevance_scores]  # Apply reduced scaling\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix after filtering\n",
    "lambda_param = 0.7  # Increased penalty for further limiting selected features\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 2, 4, 5]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 3 for score in relevance_scores]  # Reduced scaling factor\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Optional: Filter out features with an even higher minimum mutual information score\n",
    "min_relevance_threshold = 0.2  # Higher threshold to focus on top-relevance features\n",
    "filtered_features = [col for col, score in zip(features.columns, relevance_scores) if score > min_relevance_threshold]\n",
    "features = features[filtered_features]\n",
    "\n",
    "# Re-calculate mutual information scores after filtering\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 3 for score in relevance_scores]  # Apply reduced scaling\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix after filtering\n",
    "lambda_param = 0.8  # Further increased penalty for more stringent selection\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 5, 6, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'sport_binned', 'ttl_diff',\n",
      "       'window_diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (e.g., mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 3 for score in relevance_scores]  # Scaling by 3\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Optional: Filter out features with an intermediate minimum relevance score\n",
    "min_relevance_threshold = 0.18  # Set a slightly lower threshold\n",
    "filtered_features = [col for col, score in zip(features.columns, relevance_scores) if score > min_relevance_threshold]\n",
    "features = features[filtered_features]\n",
    "\n",
    "# Re-calculate mutual information scores after filtering\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 3 for score in relevance_scores]  # Apply reduced scaling\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix after filtering\n",
    "lambda_param = 0.6  # Intermediate penalty to target 8-10 features\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Only set diagonal terms for relevance and feature count penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: Lambda: 0.5, Relevance Threshold: 0.15\n",
      "Selected Feature Indices: [1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'swin', 'dwin', 'attack_cat', 'proto_state_interaction',\n",
      "       'sport_binned', 'dsport_binned', 'ttl_diff', 'window_diff',\n",
      "       'byte_packet_ratio_src'],\n",
      "      dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.5, Relevance Threshold: 0.18\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'swin', 'dwin', 'attack_cat', 'proto_state_interaction',\n",
      "       'sport_binned', 'dsport_binned', 'ttl_diff', 'window_diff'],\n",
      "      dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.5, Relevance Threshold: 0.2\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'sport_binned', 'ttl_diff',\n",
      "       'window_diff'],\n",
      "      dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.6, Relevance Threshold: 0.15\n",
      "Selected Feature Indices: [1, 2, 7, 8, 11, 12]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'sport_binned', 'ttl_diff',\n",
      "       'window_diff'],\n",
      "      dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.6, Relevance Threshold: 0.18\n",
      "Selected Feature Indices: [0, 1, 5, 6, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'sport_binned', 'ttl_diff',\n",
      "       'window_diff'],\n",
      "      dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.6, Relevance Threshold: 0.2\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4, 5]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'sport_binned', 'ttl_diff',\n",
      "       'window_diff'],\n",
      "      dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.7, Relevance Threshold: 0.15\n",
      "Selected Feature Indices: [1, 2, 7, 11, 12]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.7, Relevance Threshold: 0.18\n",
      "Selected Feature Indices: [0, 1, 5, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.7, Relevance Threshold: 0.2\n",
      "Selected Feature Indices: [0, 1, 2, 4, 5]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']\n",
    "target_column = 'Label'\n",
    "target = df[target_column]\n",
    "\n",
    "# Define ranges for lambda penalty and relevance threshold around Experiment 6 and 7 values\n",
    "lambda_values = [0.5, 0.6, 0.7]  # Around the Experiment 6 and 7 values\n",
    "relevance_thresholds = [0.15, 0.18, 0.2]  # Slightly above and below previous experiments\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}\n",
    "\n",
    "# Iterate over combinations of lambda and relevance threshold\n",
    "for lambda_param in lambda_values:\n",
    "    for min_relevance_threshold in relevance_thresholds:\n",
    "        # Step 1: Filter and select relevant features based on threshold\n",
    "        relevance_scores = mutual_info_classif(df.drop(columns=ignored_columns), target)\n",
    "        scaled_relevance_scores = [score * 3 for score in relevance_scores]\n",
    "        filtered_features = [col for col, score in zip(df.drop(columns=ignored_columns).columns, relevance_scores) if score > min_relevance_threshold]\n",
    "        features = df[filtered_features]\n",
    "\n",
    "        # Recalculate mutual information scores after filtering\n",
    "        relevance_scores = mutual_info_classif(features, target)\n",
    "        scaled_relevance_scores = [score * 3 for score in relevance_scores]\n",
    "        feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "        # Initialize QUBO matrix\n",
    "        n_features = len(features.columns)\n",
    "        qubo = np.zeros((n_features, n_features))\n",
    "        \n",
    "        # Apply penalties\n",
    "        for i in range(n_features):\n",
    "            qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "        # Convert QUBO to dictionary format\n",
    "        qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "        \n",
    "        # Solve QUBO with Quantum Annealer\n",
    "        sampler = EmbeddingComposite(DWaveSampler())\n",
    "        sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "        # Extract selected features\n",
    "        best_solution = sampleset.first.sample\n",
    "        selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "        selected_feature_names = features.columns[selected_features]\n",
    "        \n",
    "        # Store result for this configuration\n",
    "        config_name = f\"Lambda: {lambda_param}, Relevance Threshold: {min_relevance_threshold}\"\n",
    "        results[config_name] = {\n",
    "            'Selected Feature Indices': selected_features,\n",
    "            'Selected Feature Names': selected_feature_names\n",
    "        }\n",
    "\n",
    "# Display the results\n",
    "for config, result in results.items():\n",
    "    print(f\"\\nConfiguration: {config}\")\n",
    "    print(\"Selected Feature Indices:\", result['Selected Feature Indices'])\n",
    "    print(\"Selected Feature Names:\", result['Selected Feature Names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Feature Indices: [0, 1, 5, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']  # Add any other columns to ignore here\n",
    "features = df.drop(columns=ignored_columns)  # Keep only relevant columns\n",
    "target = df['Label']\n",
    "\n",
    "# Step 1: Calculate Feature Relevance (mutual information scores)\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 3 for score in relevance_scores]  # Scaling by 3\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Optional: Filter out features with a minimum relevance score\n",
    "min_relevance_threshold = 0.18  # Adjust threshold as needed\n",
    "filtered_features = [col for col, score in zip(features.columns, relevance_scores) if score > min_relevance_threshold]\n",
    "features = features[filtered_features]\n",
    "\n",
    "# Re-calculate mutual information scores after filtering\n",
    "relevance_scores = mutual_info_classif(features, target)\n",
    "scaled_relevance_scores = [score * 3 for score in relevance_scores]\n",
    "feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "# Set QUBO Parameters\n",
    "n_features = len(features.columns)  # Size of the QUBO matrix after filtering\n",
    "lambda_param = 0.6  # Penalty for selecting too many features\n",
    "alpha = 0.3  # Penalty scaling factor for redundancy\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo = np.zeros((n_features, n_features))\n",
    "\n",
    "# Populate the QUBO matrix\n",
    "# Diagonal terms: Relevance and selection penalty\n",
    "for i in range(n_features):\n",
    "    qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "# Off-diagonal terms: Redundancy penalty based on correlation\n",
    "correlation_matrix = features.corr().abs()\n",
    "for i in range(n_features):\n",
    "    for j in range(i + 1, n_features):\n",
    "        redundancy_penalty = alpha * correlation_matrix.iloc[i, j]\n",
    "        qubo[i, j] = redundancy_penalty\n",
    "        qubo[j, i] = redundancy_penalty  # QUBO matrix is symmetric\n",
    "\n",
    "# Convert QUBO matrix to dictionary format for D-Wave\n",
    "qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "\n",
    "# Step 3: Solve QUBO with Quantum Annealer\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "# Extract selected features from the best solution\n",
    "best_solution = sampleset.first.sample\n",
    "selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "\n",
    "# Display selected features\n",
    "selected_feature_names = features.columns[selected_features]\n",
    "print(\"\\nSelected Feature Indices:\", selected_features)\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: Lambda: 0.75, Relevance Threshold: 0.18\n",
      "Selected Feature Indices: [0, 1, 5, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.75, Relevance Threshold: 0.2\n",
      "Selected Feature Indices: [0, 1, 2, 4, 5]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.75, Relevance Threshold: 0.22\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.8, Relevance Threshold: 0.18\n",
      "Selected Feature Indices: [0, 1, 5, 8, 9]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.8, Relevance Threshold: 0.2\n",
      "Selected Feature Indices: [0, 1, 2, 4, 5]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.8, Relevance Threshold: 0.22\n",
      "Selected Feature Indices: [0, 1, 2, 3, 4]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'proto_state_interaction', 'ttl_diff', 'window_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.85, Relevance Threshold: 0.18\n",
      "Selected Feature Indices: [0, 1, 8]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'ttl_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.85, Relevance Threshold: 0.2\n",
      "Selected Feature Indices: [0, 1, 4]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'ttl_diff'], dtype='object')\n",
      "\n",
      "Configuration: Lambda: 0.85, Relevance Threshold: 0.22\n",
      "Selected Feature Indices: [0, 1, 3]\n",
      "Selected Feature Names: Index(['sttl', 'dttl', 'ttl_diff'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler\n",
    "import dimod\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/Scaled_UNSW_NB15_Imputed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to ignore (e.g., IP addresses, non-numeric features)\n",
    "ignored_columns = ['srcip', 'dstip', 'Label']\n",
    "target_column = 'Label'\n",
    "target = df[target_column]\n",
    "\n",
    "# Define ranges for lambda penalty and relevance threshold around Experiment 8 values\n",
    "lambda_values = [0.75, 0.8, 0.85]  # Around Experiment 8â€™s lambda\n",
    "relevance_thresholds = [0.18, 0.2, 0.22]  # Around Experiment 8â€™s relevance threshold\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}\n",
    "\n",
    "# Iterate over combinations of lambda and relevance threshold\n",
    "for lambda_param in lambda_values:\n",
    "    for min_relevance_threshold in relevance_thresholds:\n",
    "        # Step 1: Filter and select relevant features based on threshold\n",
    "        relevance_scores = mutual_info_classif(df.drop(columns=ignored_columns), target)\n",
    "        scaled_relevance_scores = [score * 3 for score in relevance_scores]\n",
    "        filtered_features = [col for col, score in zip(df.drop(columns=ignored_columns).columns, relevance_scores) if score > min_relevance_threshold]\n",
    "        features = df[filtered_features]\n",
    "\n",
    "        # Recalculate mutual information scores after filtering\n",
    "        relevance_scores = mutual_info_classif(features, target)\n",
    "        scaled_relevance_scores = [score * 3 for score in relevance_scores]\n",
    "        feature_importance = {i: score for i, score in enumerate(scaled_relevance_scores)}\n",
    "\n",
    "        # Initialize QUBO matrix\n",
    "        n_features = len(features.columns)\n",
    "        qubo = np.zeros((n_features, n_features))\n",
    "        \n",
    "        # Apply penalties\n",
    "        for i in range(n_features):\n",
    "            qubo[i, i] = lambda_param - feature_importance[i]\n",
    "\n",
    "        # Convert QUBO to dictionary format\n",
    "        qubo_dict = {(i, j): qubo[i, j] for i in range(n_features) for j in range(i, n_features)}\n",
    "        \n",
    "        # Solve QUBO with Quantum Annealer\n",
    "        sampler = EmbeddingComposite(DWaveSampler())\n",
    "        sampleset = sampler.sample_qubo(qubo_dict, num_reads=100)\n",
    "\n",
    "        # Extract selected features\n",
    "        best_solution = sampleset.first.sample\n",
    "        selected_features = [i for i, value in best_solution.items() if value == 1]\n",
    "        selected_feature_names = features.columns[selected_features]\n",
    "        \n",
    "        # Store result for this configuration\n",
    "        config_name = f\"Lambda: {lambda_param}, Relevance Threshold: {min_relevance_threshold}\"\n",
    "        results[config_name] = {\n",
    "            'Selected Feature Indices': selected_features,\n",
    "            'Selected Feature Names': selected_feature_names\n",
    "        }\n",
    "\n",
    "# Display the results\n",
    "for config, result in results.items():\n",
    "    print(f\"\\nConfiguration: {config}\")\n",
    "    print(\"Selected Feature Indices:\", result['Selected Feature Indices'])\n",
    "    print(\"Selected Feature Names:\", result['Selected Feature Names'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
