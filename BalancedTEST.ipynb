{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (2059418, 77)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(\"Shape of the dataset:\", data.shape)\n",
    "\n",
    "# Plot the distribution of the target variable (Label)\n",
    "plt.figure(figsize=(8, 6))\n",
    "data['Label'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title(\"Distribution of Classes in the Dataset\")\n",
    "plt.xlabel(\"Class (0 = Normal, 1 = Attack)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best F1 Score from Grid Search: 0.9929442368379744\n",
      "Validation Accuracy: 1.0\n",
      "Validation F1 Score: 1.0\n",
      "Validation Precision: 1.0\n",
      "Validation Recall: 1.0\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    587936\n",
      "           1       1.00      1.00      1.00     29890\n",
      "\n",
      "    accuracy                           1.00    617826\n",
      "   macro avg       1.00      1.00      1.00    617826\n",
      "weighted avg       1.00      1.00      1.00    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the full dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Remove 'srcip', 'dstip', and 'attack_cat' columns\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Sample a subset of the data for grid search to reduce memory usage\n",
    "sample_data = data.sample(n=200000, random_state=42)  # Sample 200,000 rows\n",
    "X_sample = sample_data.drop('Label', axis=1)\n",
    "y_sample = sample_data['Label']\n",
    "\n",
    "# Define the RandomForest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees\n",
    "    'max_depth': [10, 20, None],      # Max depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Min samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],    # Min samples required at a leaf node\n",
    "    'class_weight': ['balanced', 'balanced_subsample']  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the sampled training data\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Retrieve the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1 Score from Grid Search:\", best_score)\n",
    "\n",
    "# Train the model with the best parameters on the full dataset\n",
    "best_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_model.fit(X, y)  # Train on the full data with the best parameters\n",
    "\n",
    "# Split full dataset into training and validation for final evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_preds = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "val_f1 = f1_score(y_val, val_preds, average='weighted')\n",
    "val_precision = precision_score(y_val, val_preds, average='weighted')\n",
    "val_recall = recall_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "# Print validation results\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation F1 Score:\", val_f1)\n",
    "print(\"Validation Precision:\", val_precision)\n",
    "print(\"Validation Recall:\", val_recall)\n",
    "print(\"\\nClassification Report (Validation Set):\\n\", classification_report(y_val, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (2059418, 74)\n",
      "Training Time (seconds): 706.9816617965698\n",
      "Testing Time (seconds): 6.890068531036377\n",
      "Training Accuracy: 0.9999993063224546\n",
      "Validation Accuracy: 0.9947687536620343\n",
      "Validation F1 Score: 0.9947349684194035\n",
      "Validation Precision: 0.9947189795322431\n",
      "Validation Recall: 0.9947687536620343\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    587936\n",
      "           1       0.96      0.93      0.95     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.98      0.97      0.97    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n",
      "No significant overfitting detected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the full dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Remove 'srcip', 'dstip', and 'attack_cat' columns\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Display the shape of the dataset before training\n",
    "print(\"Shape of the dataset:\", data.shape)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the RandomForest model with best parameters from Grid Search\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    class_weight='balanced',\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Start the timer for training time\n",
    "start_training_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# End the timer for training time\n",
    "end_training_time = time.time()\n",
    "training_time = end_training_time - start_training_time\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_preds = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Start the timer for testing time\n",
    "start_testing_time = time.time()\n",
    "\n",
    "# Predict on the validation set\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# End the timer for testing time\n",
    "end_testing_time = time.time()\n",
    "testing_time = end_testing_time - start_testing_time\n",
    "\n",
    "# Calculate validation accuracy and other metrics\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "val_f1 = f1_score(y_val, val_preds, average='weighted')\n",
    "val_precision = precision_score(y_val, val_preds, average='weighted')\n",
    "val_recall = recall_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "# Output results\n",
    "print(f\"Training Time (seconds): {training_time}\")\n",
    "print(f\"Testing Time (seconds): {testing_time}\")\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation F1 Score: {val_f1}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "\n",
    "# Classification Report for additional insights\n",
    "print(\"\\nClassification Report (Validation Set):\\n\", classification_report(y_val, val_preds))\n",
    "\n",
    "# Check for overfitting\n",
    "if train_accuracy - val_accuracy > 0.1:\n",
    "    print(\"Warning: Potential overfitting detected. Consider tuning the model.\")\n",
    "else:\n",
    "    print(\"No significant overfitting detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search...\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Best F1 Score from Grid Search: 0.992212310676783\n",
      "Accuracy on Test Set: 0.9918666666666667\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     14279\n",
      "           1       0.91      0.92      0.92       721\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.95      0.96      0.96     15000\n",
      "weighted avg       0.99      0.99      0.99     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Load the full dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Reduce the dataset size for faster execution and to prevent memory issues\n",
    "data = data.sample(n=50000, random_state=42)  # Adjust sample size as needed\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model with tree_method set to 'hist' for CPU usage\n",
    "model = XGBClassifier(use_label_encoder=False, tree_method='hist', random_state=42)\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with error_score='raise' to identify failed fits\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'  # Raises an error if a combination fails, for debugging\n",
    ")\n",
    "\n",
    "# Run Grid Search\n",
    "print(\"Running Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and F1 score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score from Grid Search:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Accuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "Accuracy on Test Set: 0.9934188590315073\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    587936\n",
      "           1       0.93      0.93      0.93     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.97      0.96      0.96    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the full dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize XGBoost with best parameters from Grid Search\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0,\n",
    "    'use_label_encoder': False,\n",
    "    'tree_method': 'hist',  # Optimized for CPU training\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model = XGBClassifier(**best_params)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XGBoost model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on Test Set:\", accuracy)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 Permutation Importance Features: ['ct_state_ttl', 'packet_count_change_rate', 'ct_srv_dst', 'ct_dst_src_ltm', 'dbytes', 'byte_count_change_rate']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model for permutation importance\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate Permutation Importance\n",
    "perm_importance = permutation_importance(model, X_train, y_train, n_repeats=10, random_state=42)\n",
    "perm_features = pd.Series(perm_importance.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Select the top 6 features based on Permutation Importance\n",
    "top_6_perm_features = perm_features.head(6).index.tolist()\n",
    "print(\"Top 6 Permutation Importance Features:\", top_6_perm_features)\n",
    "\n",
    "# Save the reduced dataset using only the selected features\n",
    "X_train_perm_selected = X_train[top_6_perm_features]\n",
    "X_test_perm_selected = X_test[top_6_perm_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Accuracy: 0.990549119007617\n",
      "F1 Score: 0.9905927083723889\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    587936\n",
      "           1       0.89      0.91      0.90     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.95      0.95      0.95    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Select only the top 6 permutation importance features\n",
    "X = data[['ct_state_ttl', 'packet_count_change_rate', 'ct_srv_dst', 'ct_dst_src_ltm', 'dbytes', 'byte_count_change_rate']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the RandomForest model with controlled parallelism\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,       # Original parameter\n",
    "    max_depth=None,         # Original parameter\n",
    "    min_samples_split=2,    # Original parameter\n",
    "    min_samples_leaf=1,     # Original parameter\n",
    "    class_weight='balanced',\n",
    "    n_jobs=4,               # Controlled parallelism to reduce resource strain\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy: 0.9890454594011906\n",
      "F1 Score: 0.9891695622795099\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    587936\n",
      "           1       0.87      0.91      0.89     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.93      0.95      0.94    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Select only the top 6 permutation importance features\n",
    "X = data[['ct_state_ttl', 'packet_count_change_rate', 'ct_srv_dst', 'ct_dst_src_ltm', 'dbytes', 'byte_count_change_rate']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model with the best parameters and controlled parallelism\n",
    "xgb_model = XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0,\n",
    "    use_label_encoder=False,  # Newer versions of XGBoost require this\n",
    "    eval_metric='logloss',    # Specify an eval metric to suppress warnings\n",
    "    n_jobs=4,                 # Controlled parallelism to prevent resource strain\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 Mutual Information Features: ['sttl', 'ttl_diff', 'dttl', 'proto_state_interaction', 'window_diff', 'sport_binned']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calculate Mutual Information\n",
    "mutual_info_values = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "mutual_info_features = pd.Series(mutual_info_values, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Select the top 6 features based on Mutual Information\n",
    "top_6_mutual_info_features = mutual_info_features.head(6).index.tolist()\n",
    "print(\"Top 6 Mutual Information Features:\", top_6_mutual_info_features)\n",
    "\n",
    "# Save the reduced dataset using only the selected features\n",
    "X_train_mutual_info_selected = X_train[top_6_mutual_info_features]\n",
    "X_test_mutual_info_selected = X_test[top_6_mutual_info_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Accuracy: 0.9859232211010868\n",
      "F1 Score: 0.9867626860245129\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    587936\n",
      "           1       0.77      1.00      0.87     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.89      0.99      0.93    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Select only the top 6 Mutual Information features\n",
    "X = data[['sttl', 'ttl_diff', 'dttl', 'proto_state_interaction', 'window_diff', 'sport_binned']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model with the best hyperparameters found previously\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy: 0.9858827566337448\n",
      "F1 Score: 0.9867216944591903\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    587936\n",
      "           1       0.77      1.00      0.87     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.89      0.99      0.93    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Select only the top 6 Mutual Information features\n",
    "X = data[['sttl', 'ttl_diff', 'dttl', 'proto_state_interaction', 'window_diff', 'sport_binned']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model with the best hyperparameters found previously\n",
    "xgb_model = XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain (IG) Scores for Features:\n",
      "                    Feature  Information Gain Score\n",
      "3                      sttl                0.396694\n",
      "4                      dttl                0.379538\n",
      "55                 ttl_diff                0.360198\n",
      "40  proto_state_interaction                0.280599\n",
      "57              window_diff                0.274566\n",
      "..                      ...                     ...\n",
      "32               ct_ftp_cmd                0.001346\n",
      "31             is_ftp_login                0.001185\n",
      "42          service_grouped                0.000263\n",
      "41            proto_grouped                0.000222\n",
      "28          is_sm_ips_ports                0.000122\n",
      "\n",
      "[73 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Define columns to ignore\n",
    "columns_to_ignore = ['srcip', 'dstip', 'attack_cat']\n",
    "df = df.drop(columns=columns_to_ignore)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Calculate Information Gain (IG) scores\n",
    "ig_scores = mutual_info_classif(X, y)\n",
    "ig_scores_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Information Gain Score': ig_scores\n",
    "}).sort_values(by='Information Gain Score', ascending=False)\n",
    "\n",
    "# Display Information Gain scores\n",
    "print(\"Information Gain (IG) Scores for Features:\")\n",
    "print(ig_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix for Features (with thresholding):\n",
      "                              dur    sbytes    dbytes      sttl      dttl  \\\n",
      "dur                       1.00000  0.000000  0.106980  0.000000  0.000000   \n",
      "sbytes                    0.00000  1.000000  0.000000  0.000000  0.060840   \n",
      "dbytes                    0.10698  0.000000  1.000000 -0.058008  0.000000   \n",
      "sttl                      0.00000  0.000000 -0.058008  1.000000  0.506621   \n",
      "dttl                      0.00000  0.060840  0.000000  0.506621  1.000000   \n",
      "...                           ...       ...       ...       ...       ...   \n",
      "day_of_week               0.00000  0.000000  0.000000 -0.177299 -0.119634   \n",
      "Sintpkt_var               0.00000  0.000000  0.000000  0.000000  0.000000   \n",
      "Dintpkt_var               0.00000  0.000000  0.000000  0.000000  0.000000   \n",
      "packet_count_change_rate  0.00000  0.000000  0.197294  0.000000  0.000000   \n",
      "byte_count_change_rate    0.00000  0.052131  0.000000  0.000000  0.000000   \n",
      "\n",
      "                             sloss     dloss     Sload     Dload     Spkts  \\\n",
      "dur                       0.053365  0.107924  0.000000  0.000000  0.106264   \n",
      "sbytes                    0.959444  0.000000  0.000000  0.000000  0.620005   \n",
      "dbytes                    0.101969  0.991620  0.000000  0.066749  0.714169   \n",
      "sttl                      0.000000 -0.072279  0.315422 -0.163978 -0.077342   \n",
      "dttl                      0.000000  0.000000 -0.094201 -0.111144  0.000000   \n",
      "...                            ...       ...       ...       ...       ...   \n",
      "day_of_week               0.000000  0.000000 -0.080732  0.000000  0.000000   \n",
      "Sintpkt_var               0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "Dintpkt_var               0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "packet_count_change_rate  0.107692  0.222092  0.000000  0.184782  0.252765   \n",
      "byte_count_change_rate    0.056750  0.000000  0.000000  0.000000  0.050783   \n",
      "\n",
      "                          ...  session_duration  byte_entropy  Sintpkt_std  \\\n",
      "dur                       ...          0.591587      0.000000     0.056328   \n",
      "sbytes                    ...          0.000000     -0.066106     0.000000   \n",
      "dbytes                    ...          0.062021     -0.338191     0.000000   \n",
      "sttl                      ...          0.000000     -0.211944     0.000000   \n",
      "dttl                      ...          0.000000      0.081612     0.000000   \n",
      "...                       ...               ...           ...          ...   \n",
      "day_of_week               ...          0.000000      0.054874     0.000000   \n",
      "Sintpkt_var               ...          0.000000      0.000000     0.771029   \n",
      "Dintpkt_var               ...          0.000000      0.000000     0.428011   \n",
      "packet_count_change_rate  ...          0.000000     -0.180828     0.000000   \n",
      "byte_count_change_rate    ...          0.000000      0.000000     0.000000   \n",
      "\n",
      "                          Dintpkt_std      hour  day_of_week  Sintpkt_var  \\\n",
      "dur                          0.067892  0.000000     0.000000       0.0000   \n",
      "sbytes                       0.000000  0.000000     0.000000       0.0000   \n",
      "dbytes                       0.000000  0.000000     0.000000       0.0000   \n",
      "sttl                         0.000000 -0.168156    -0.177299       0.0000   \n",
      "dttl                         0.000000 -0.116447    -0.119634       0.0000   \n",
      "...                               ...       ...          ...          ...   \n",
      "day_of_week                  0.000000  0.693222     1.000000       0.0000   \n",
      "Sintpkt_var                  0.257129  0.000000     0.000000       1.0000   \n",
      "Dintpkt_var                  0.742812  0.000000     0.000000       0.3442   \n",
      "packet_count_change_rate     0.000000  0.000000     0.000000       0.0000   \n",
      "byte_count_change_rate       0.000000  0.000000     0.000000       0.0000   \n",
      "\n",
      "                          Dintpkt_var  packet_count_change_rate  \\\n",
      "dur                            0.0000                  0.000000   \n",
      "sbytes                         0.0000                  0.000000   \n",
      "dbytes                         0.0000                  0.197294   \n",
      "sttl                           0.0000                  0.000000   \n",
      "dttl                           0.0000                  0.000000   \n",
      "...                               ...                       ...   \n",
      "day_of_week                    0.0000                  0.000000   \n",
      "Sintpkt_var                    0.3442                  0.000000   \n",
      "Dintpkt_var                    1.0000                  0.000000   \n",
      "packet_count_change_rate       0.0000                  1.000000   \n",
      "byte_count_change_rate         0.0000                  0.569467   \n",
      "\n",
      "                          byte_count_change_rate  \n",
      "dur                                     0.000000  \n",
      "sbytes                                  0.052131  \n",
      "dbytes                                  0.000000  \n",
      "sttl                                    0.000000  \n",
      "dttl                                    0.000000  \n",
      "...                                          ...  \n",
      "day_of_week                             0.000000  \n",
      "Sintpkt_var                             0.000000  \n",
      "Dintpkt_var                             0.000000  \n",
      "packet_count_change_rate                0.569467  \n",
      "byte_count_change_rate                  1.000000  \n",
      "\n",
      "[73 rows x 73 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\AppData\\Local\\Temp\\ipykernel_19976\\1686654484.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  correlation_matrix = correlation_matrix.applymap(lambda x: x if abs(x) >= correlation_threshold else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is loaded and preprocessed as in Part 1\n",
    "# Calculate correlation matrix for the features\n",
    "correlation_matrix = df.drop(columns=['Label']).corr()\n",
    "\n",
    "# Optional: Set a correlation threshold for filtering\n",
    "correlation_threshold = 0.05  # Adjust based on your needs\n",
    "correlation_matrix = correlation_matrix.applymap(lambda x: x if abs(x) >= correlation_threshold else 0)\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix for Features (with thresholding):\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optional: Save correlation matrix for future reference\n",
    "correlation_matrix.to_csv(\"correlation_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ig_scores_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m    \u001b[38;5;66;03m# Weight for correlations on the off-diagonal\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract features based on IG scores and ensure alignment with correlation matrix\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mig_scores_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     14\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_features)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize the QUBO matrix\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ig_scores_df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `ig_scores_df` and `correlation_matrix` are already defined and loaded\n",
    "# ig_scores_df should have columns: 'Feature', 'Information Gain Score'\n",
    "# correlation_matrix should be a DataFrame where columns and indices are feature names\n",
    "\n",
    "# Define lambda and alpha parameters for QUBO weighting\n",
    "lambda_ = 1  # Weight for IG scores on the diagonal\n",
    "alpha = 0.3    # Weight for correlations on the off-diagonal\n",
    "\n",
    "# Extract features based on IG scores and ensure alignment with correlation matrix\n",
    "selected_features = ig_scores_df['Feature'].tolist()\n",
    "n_features = len(selected_features)\n",
    "\n",
    "# Initialize the QUBO matrix\n",
    "qubo_matrix = np.zeros((n_features, n_features))\n",
    "\n",
    "# Populate the diagonal with Information Gain scores, scaled by lambda\n",
    "for i, feature in enumerate(selected_features):\n",
    "    ig_score = ig_scores_df.loc[ig_scores_df['Feature'] == feature, 'Information Gain Score'].values[0]\n",
    "    qubo_matrix[i, i] = lambda_ - ig_score\n",
    "\n",
    "# Populate the off-diagonal with correlation values, scaled by alpha\n",
    "for i in range(n_features):\n",
    "    for j in range(i + 1, n_features):\n",
    "        feature_i = selected_features[i]\n",
    "        feature_j = selected_features[j]\n",
    "        if feature_i in correlation_matrix.index and feature_j in correlation_matrix.columns:\n",
    "            correlation_value = correlation_matrix.loc[feature_i, feature_j]\n",
    "            qubo_matrix[i, j] = alpha * correlation_value\n",
    "            qubo_matrix[j, i] = qubo_matrix[i, j]  # Ensure symmetry\n",
    "\n",
    "# Convert QUBO matrix to a DataFrame for readability\n",
    "qubo_df = pd.DataFrame(qubo_matrix, index=selected_features, columns=selected_features)\n",
    "\n",
    "# Display QUBO matrix\n",
    "print(\"Constructed QUBO Matrix:\")\n",
    "print(qubo_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample results:\n",
      "    0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 ... 72   energy num_oc. ...\n",
      "0   1  0  0  0  0  0  0  0  0  1  0  0  0  1  0  0 ...  0  1.57652       1 ...\n",
      "1   0  1  0  0  1  0  0  0  0  0  0  0  0  1  0  0 ...  0 1.663894       1 ...\n",
      "2   0  1  1  0  0  0  0  0  0  0  0  0  0  1  1  1 ...  0 1.841113       1 ...\n",
      "3   0  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0 ...  0 2.077656       1 ...\n",
      "4   0  1  0  1  1  0  0  0  0  0  0  0  0  0  1  1 ...  0 2.351025       1 ...\n",
      "5   1  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0 ...  0 2.367371       1 ...\n",
      "6   0  1  0  0  1  0  1  0  0  0  1  0  0  1  0  0 ...  0 2.437742       1 ...\n",
      "7   0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0 ...  0 2.452315       1 ...\n",
      "8   1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ...  0 2.462293       1 ...\n",
      "9   0  1  0  0  0  1  0  0  0  0  0  0  0  1  0  0 ...  0 2.729988       1 ...\n",
      "10  0  1  0  1  0  0  1  0  0  0  0  0  0  1  0  0 ...  0 2.761367       1 ...\n",
      "11  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1 ...  0 2.962925       1 ...\n",
      "12  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1 ...  0 3.013302       1 ...\n",
      "13  1  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0 ...  0 3.124304       1 ...\n",
      "14  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0 ...  0  3.13021       1 ...\n",
      "15  0  1  0  1  1  1  1  0  0  0  0  0  0  0  0  0 ...  0 3.162412       1 ...\n",
      "71  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.170465       1 ...\n",
      "16  1  0  0  1  0  0  0  0  0  1  0  0  0  1  1  0 ...  0 3.194337       1 ...\n",
      "17  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.198212       1 ...\n",
      "18  0  1  0  0  0  0  1  0  0  0  0  0  0  1  1  0 ...  0 3.198759       1 ...\n",
      "19  1  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0 ...  0 3.210519       1 ...\n",
      "20  0  1  0  1  1  1  0  0  1  0  0  0  0  0  1  1 ...  0 3.241306       1 ...\n",
      "21  0  1  0  0  0  0  0  0  1  1  1  0  0  1  0  0 ...  0 3.264298       1 ...\n",
      "22  0  0  1  0  0  0  1  1  0  0  0  0  0  1  0  1 ...  0 3.371459       1 ...\n",
      "23  0  0  0  0  0  0  0  1  1  1  0  1  0  0  0  0 ...  0 3.393794       1 ...\n",
      "24  1  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.426492       1 ...\n",
      "25  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0 ...  1 3.473797       1 ...\n",
      "26  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.567086       1 ...\n",
      "27  0  1  1  1  0  0  1  0  0  0  0  0  0  1  0  0 ...  1 3.586079       1 ...\n",
      "28  0  0  1  0  0  0  0  1  0  0  0  0  0  1  0  0 ...  0 3.593471       1 ...\n",
      "29  1  1  0  0  0  0  0  0  1  1  0  0  1  0  0  0 ...  0  3.61493       1 ...\n",
      "30  1  0  0  0  1  0  0  0  0  0  0  0  0  1  1  0 ...  0 3.619255       1 ...\n",
      "31  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0 ...  0 3.623825       1 ...\n",
      "32  1  1  0  0  0  0  0  0  1  0  0  0  0  0  1  1 ...  1 3.643857       1 ...\n",
      "33  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0 ...  0 3.647473       1 ...\n",
      "34  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  1 ...  0 3.669772       1 ...\n",
      "35  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ...  0 3.672836       1 ...\n",
      "36  0  1  1  0  0  0  0  1  0  0  0  0  0  1  0  0 ...  0 3.722911       1 ...\n",
      "37  0  1  0  0  0  0  1  0  1  0  0  0  0  1  0  0 ...  0 3.731363       1 ...\n",
      "91  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0 ...  0 3.773044       1 ...\n",
      "38  0  1  0  1  0  0  0  1  1  0  0  0  0  1  0  0 ...  0 3.783754       1 ...\n",
      "39  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  1 ...  0 3.791297       1 ...\n",
      "40  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.861388       1 ...\n",
      "41  0  0  0  1  0  0  0  0  0  0  0  1  0  1  0  0 ...  0 3.868118       1 ...\n",
      "42  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.869088       1 ...\n",
      "43  1  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.877903       1 ...\n",
      "44  0  0  0  0  0  0  0  1  1  1  1  1  0  1  1  1 ...  0 3.907839       1 ...\n",
      "87  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 3.908058       1 ...\n",
      "45  1  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0 ...  0 3.943405       1 ...\n",
      "46  0  1  0  1  0  1  1  0  0  0  0  0  0  0  0  0 ...  0 3.958334       1 ...\n",
      "47  1  1  0  0  0  0  0  0  1  1  0  0  0  1  1  0 ...  1  3.97497       1 ...\n",
      "48  0  0  1  1  0  0  1  0  1  0  0  0  0  1  0  0 ...  0  4.03272       1 ...\n",
      "49  0  1  1  0  0  1  0  0  0  0  0  1  0  1  0  1 ...  0 4.048281       1 ...\n",
      "50  0  0  0  0  0  1  0  0  0  0  1  0  0  1  0  0 ...  0 4.053988       1 ...\n",
      "51  0  1  1  1  0  0  1  0  1  0  0  0  0  0  0  0 ...  1 4.060818       1 ...\n",
      "52  0  0  0  1  0  1  1  0  0  0  0  0  0  1  0  0 ...  0 4.099351       1 ...\n",
      "53  0  1  0  1  1  0  0  1  0  0  0  0  0  1  1  0 ...  0 4.112791       1 ...\n",
      "54  0  1  0  1  1  0  0  0  0  0  0  0  0  0  1  0 ...  0 4.129931       1 ...\n",
      "55  0  0  1  1  0  0  1  0  0  0  0  0  0  0  0  0 ...  0 4.168149       1 ...\n",
      "56  1  0  0  1  0  0  1  0  1  0  0  0  0  1  1  0 ...  0 4.200764       1 ...\n",
      "57  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 4.266576       1 ...\n",
      "58  1  1  0  0  1  0  0  0  0  0  0  1  0  1  0  0 ...  0 4.301045       1 ...\n",
      "59  0  0  0  1  0  0  0  0  0  0  0  1  0  1  0  0 ...  0 4.308078       1 ...\n",
      "60  0  1  1  0  0  0  1  0  1  0  0  1  0  1  1  0 ...  0 4.349347       1 ...\n",
      "61  1  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0 ...  0 4.407437       1 ...\n",
      "95  0  1  0  0  1  0  0  1  1  0  0  0  0  1  0  0 ...  0 4.425693       1 ...\n",
      "62  0  1  0  0  0  0  1  0  0  0  0  0  0  1  0  0 ...  1 4.432044       1 ...\n",
      "63  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  1 ...  0 4.490927       1 ...\n",
      "64  0  1  0  0  1  0  0  0  0  0  0  1  0  0  1  0 ...  0 4.516848       1 ...\n",
      "65  0  1  0  0  0  0  0  0  1  1  0  0  0  0  1  0 ...  0 4.524594       1 ...\n",
      "66  0  0  0  0  0  0  0  0  1  0  0  0  0  1  1  0 ...  1 4.538646       1 ...\n",
      "67  0  0  0  0  1  0  0  0  0  0  0  1  0  1  0  0 ...  0 4.539579       1 ...\n",
      "68  0  1  0  0  0  0  0  1  0  0  0  0  0  1  0  0 ...  0 4.542126       1 ...\n",
      "69  0  0  0  0  1  0  1  0  0  0  1  0  0  1  0  1 ...  0 4.549957       1 ...\n",
      "70  0  0  0  1  0  1  0  1  0  0  0  0  0  1  0  0 ...  0 4.554709       1 ...\n",
      "72  0  1  0  1  0  0  0  0  0  0  0  0  0  1  0  0 ...  0 4.622852       1 ...\n",
      "73  0  1  1  0  1  0  1  0  0  0  0  1  0  1  0  0 ...  0 4.639228       1 ...\n",
      "74  0  0  1  0  0  1  0  0  0  0  0  0  0  1  1  0 ...  0 4.664125       1 ...\n",
      "75  0  1  0  0  0  0  0  0  1  0  0  0  0  0  1  0 ...  0 4.697274       1 ...\n",
      "76  1  0  1  0  0  0  0  1  0  0  0  1  0  1  0  0 ...  0  4.71228       1 ...\n",
      "77  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ...  0 4.741822       1 ...\n",
      "78  0  1  0  1  0  0  0  1  0  0  0  1  0  1  0  1 ...  0 4.800252       1 ...\n",
      "79  0  1  1  1  0  1  0  0  0  0  0  0  0  1  0  0 ...  0 4.820312       1 ...\n",
      "80  0  1  0  1  0  0  0  0  0  1  0  0  0  1  0  0 ...  1   4.8277       1 ...\n",
      "81  0  0  1  0  0  0  1  0  1  0  0  0  0  0  0  0 ...  0 4.985509       1 ...\n",
      "82  0  0  0  0  0  0  0  1  1  0  0  0  0  1  1  0 ...  0 5.018879       1 ...\n",
      "83  1  1  0  0  0  1  0  0  1  0  0  0  0  1  1  0 ...  0 5.033304       1 ...\n",
      "84  1  0  0  1  0  0  0  0  1  0  0  1  0  1  1  0 ...  0 5.043357       1 ...\n",
      "85  1  0  0  1  0  1  1  0  0  0  0  0  0  1  0  0 ...  0 5.056786       1 ...\n",
      "99  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 ...  0 5.095207       1 ...\n",
      "86  1  1  0  1  0  0  0  0  0  0  0  1  0  1  1  1 ...  0 5.185675       1 ...\n",
      "88  0  0  0  0  0  0  0  0  1  0  1  0  0  0  1  0 ...  1 5.226982       1 ...\n",
      "89  0  1  0  1  0  0  0  0  0  0  1  0  0  0  0  0 ...  0 5.240006       1 ...\n",
      "90  1  0  0  1  0  0  1  0  0  0  0  0  0  0  1  1 ...  0 5.260925       1 ...\n",
      "92  0  0  1  0  0  1  0  0  0  0  0  0  0  1  1  0 ...  0 5.534818       1 ...\n",
      "93  0  0  0  0  1  0  0  0  1  0  0  0  0  1  1  0 ...  0 5.570642       1 ...\n",
      "94  0  0  0  0  0  0  1  0  0  0  1  0  0  1  0  0 ...  0 5.833326       1 ...\n",
      "96  0  0  0  1  0  1  0  0  1  0  1  0  0  0  0  0 ...  0 6.318642       1 ...\n",
      "97  0  0  0  1  0  0  0  0  1  0  0  0  0  1  1  0 ...  0 6.394974       1 ...\n",
      "98  1  0  0  0  0  1  1  0  1  0  0  1  1  0  1  1 ...  0 6.440307       1 ...\n",
      "['BINARY', 100 rows, 100 samples, 73 variables]\n",
      "\n",
      "Best Solution (minimum energy):\n",
      "0     1\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "68    0\n",
      "69    0\n",
      "70    0\n",
      "71    1\n",
      "72    0\n",
      "Name: 0, Length: 73, dtype: int8\n",
      "\n",
      "Selected feature names: ['sttl', 'byte_packet_ratio_src', 'byte_entropy', 'dbytes', 'Dload', 'service_grouped']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite\n",
    "from dwave.embedding.chain_strength import uniform_torque_compensation\n",
    "from dimod import BinaryQuadraticModel\n",
    "\n",
    "# Load QUBO matrix from the previous step\n",
    "# Assuming qubo_df is already defined as the QUBO matrix DataFrame from the previous step\n",
    "qubo_dict = {(i, j): qubo_df.iloc[i, j] for i in range(len(qubo_df)) for j in range(i, len(qubo_df))}\n",
    "\n",
    "# Define the Binary Quadratic Model (BQM)\n",
    "bqm = BinaryQuadraticModel.from_qubo(qubo_dict)\n",
    "\n",
    "# Set up the D-Wave sampler with an embedding composite\n",
    "sampler = EmbeddingComposite(DWaveSampler())\n",
    "chain_strength = uniform_torque_compensation(bqm)\n",
    "\n",
    "# Execute quantum annealing with specified parameters\n",
    "sampleset = sampler.sample(bqm, chain_strength=chain_strength, num_reads=100, label='QUBO Optimization')\n",
    "\n",
    "# Display the results\n",
    "print(\"Sample results:\")\n",
    "print(sampleset)\n",
    "\n",
    "# Convert samples to a DataFrame for better readability\n",
    "samples_df = pd.DataFrame(sampleset.record.sample, columns=sampleset.variables)\n",
    "print(\"\\nBest Solution (minimum energy):\")\n",
    "print(samples_df.iloc[0])\n",
    "\n",
    "# Extract the best solution (selected features)\n",
    "best_solution = samples_df.iloc[0]\n",
    "selected_features_indices = [i for i, value in enumerate(best_solution) if value == 1]\n",
    "selected_features_names = [qubo_df.columns[i] for i in selected_features_indices]\n",
    "print(\"\\nSelected feature names:\", selected_features_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Accuracy: 0.9885582672143937\n",
      "F1 Score: 0.9888801248976863\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    587936\n",
      "           1       0.84      0.94      0.89     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.92      0.97      0.94    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Select only the chosen features\n",
    "X = data[['sttl', 'byte_packet_ratio_src', 'byte_entropy', 'dbytes', 'Dload', 'service_grouped']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model with specified hyperparameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(f\"F1 Score: {f1_rf}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy: 0.9888091469119137\n",
      "F1 Score: 0.9889455305751652\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    587936\n",
      "           1       0.87      0.91      0.89     29890\n",
      "\n",
      "    accuracy                           0.99    617826\n",
      "   macro avg       0.93      0.95      0.94    617826\n",
      "weighted avg       0.99      0.99      0.99    617826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'C:/Users/ishan/OneDrive/Desktop/Research/Dataset/UNSW-NB15/UnBalanced_Cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(columns=['srcip', 'dstip', 'attack_cat'])\n",
    "\n",
    "# Select only the chosen features\n",
    "X = data[['sttl', 'byte_packet_ratio_src', 'byte_entropy', 'dbytes', 'Dload', 'service_grouped']]\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model with specified hyperparameters\n",
    "xgb_model = XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"Accuracy: {accuracy_xgb}\")\n",
    "print(f\"F1 Score: {f1_xgb}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
